Final Report:
# Model Context Protocol（MCP）技术报告

## 引言
**模型上下文协议（Model Context Protocol，MCP）**是由Anthropic公司于2024年11月推出的开放标准，旨在规范AI模型与外部系统、数据源的集成方式【47:3†source】。随着大型语言模型（LLM）的广泛应用，模型往往受限于与实际数据隔离的“信息孤岛”问题。传统上，每接入一个新数据源或工具都需要定制开发集成，这造成**N×M**规模的碎片化适配难题【47:3†source】。MCP通过单一通用协议取代各自为政的集成方案，建立模型与数据源之间安全的双向连接，解决了功能调用（Function Calling）等原有机制的局限【47:3†source】,。作为对LLM函数调用能力的扩展，MCP的目标是使模型能够更加高效、一致地访问外部数据与工具，为生成更相关可靠的回答提供上下文支撑。

## 历史背景
MCP的诞生植根于AI工具生态的发展演进。2023年OpenAI推出了封闭的**函数调用**接口，使模型能够按预定义模式调用外部函数，实现数据查询等操作。然而该方法专属于OpenAI平台，每个函数需要预先注册且缺乏持久对话状态【47:1†source】。同年推出的**ChatGPT插件**框架允许模型通过OpenAPI定义调用第三方服务，但每个插件都是一次性的定制集成，且仅限于ChatGPT自身使用，缺乏统一的认证和持续会话机制【47:1†source】。在MCP出现之前，开发者往往不得不针对每个模型和每个数据源编写特定连接器，这被Anthropic称为“**N×M**集成问题”【47:3†source】。**MCP协议于2024年11月25日由Anthropic发布**，定位为一个开放标准框架【47:3†source】。它借鉴了语言服务器协议（LSP）的消息流思想，采用JSON-RPC 2.0作为通信机制【47:3†source】。MCP问世后迅速引起业界关注，OpenAI和Google DeepMind等主要AI提供商在2025年初也开始支持这一标准【47:3†source】。这一动向显示了业界寻求统一AI集成接口的趋势，各方希望通过MCP降低重复开发成本，提升跨平台兼容性。

## 协议结构与工作原理
**MCP采用典型的客户端-服务器架构**，通过JSON-RPC 2.0消息在模型应用（客户端）与外部工具服务（服务器）之间进行通信【47:5†source】。在这一架构中，运行LLM的主应用（称为MCP主机，例如Claude桌面应用或聊天机器人）内部嵌入了MCP客户端组件，用于管理与外部MCP服务器的连接【47:5†source】。**MCP服务器**则由各数据源或工具提供方实现，用来暴露具体的功能接口或数据资源。启动时，客户端与服务器首先进行能力协商，互相告知支持的功能特性，然后建立**有状态的持久会话**【47:5†source】。这种会话允许模型在多轮交互中保持上下文，不同于一次性函数调用的无状态请求【47:1†source】。

MCP工作的关键流程包括**工具发现**和**工具调用**两个阶段：首先，客户端可以发送`tools/list`请求获取服务器提供的可用工具列表【47:6†source】。服务器返回每个工具的名称、描述以及参数模式（基于JSON Schema的输入/输出数据格式）【47:6†source】,【47:6†source】。通过这一机制，模型能够“发现”当前可用的操作选项。然后，当LLM决定使用某项工具时，MCP客户端会发送`tools/call`请求，指定工具名称和相应参数【47:6†source】。例如，当模型需要查询天气时，可调用名为“get_weather”的工具并提供地点参数。MCP服务器接收到调用请求后执行相应操作（如访问天气API获取数据），并将结果以结构化响应返回给客户端【47:6†source】。响应中包含执行结果的数据内容，格式可以是纯文本、JSON对象，甚至二进制资源，但都封装在预定义的`result.content`字段中返回，并附带一个标志指示是否出错【47:6†source】。客户端收到结果后，再由LLM将其整合进对话回答。整个过程中，MCP确保了**调用约定的一致性**：所有交互消息均符合JSON-RPC 2.0规范，包括标准的`id`、`method`、`params`和`result`字段【47:6†source】,【47:6†source】。值得注意的是，MCP允许模型主动解析上下文决定调用何种工具，实现了**模型自主驱动**的工具使用（在用户监督下）【47:6†source】,【47:6†source】。这种自主性结合持久对话，使复杂任务的多步工具链调用成为可能。例如，AI助理可以先用MCP从数据库检索信息，再调用邮件接口发送结果，整个链路在统一协议下顺畅衔接。

MCP与LLM函数调用机制紧密结合又各司其职：**函数调用**通常发生在模型内部推理阶段，LLM决定需要调用某函数并输出一个结构化的调用意图；而**MCP负责承载并执行该调用**，充当LLM与外部实际功能执行环境之间的桥梁,。换言之，函数调用让模型“会说”要干什么，MCP让模型“真的做”成想做的事。两者协同下，LLM的自然语言能力与外部工具的执行能力得以融合。举例来说，OpenAI的函数调用接口返回一个函数名和参数的JSON对象，而通过实现一个通用的`call_mcp_tool`函数将其对接MCP，模型就能利用MCP的通道去调用对应名称的外部工具【47:1†source】,【47:1†source】。这种组合策略表明MCP作为开放协议可以嵌入现有LLM平台，使各大模型共同使用一套工具生态成为可能。

## 技术实现细节
**数据格式与传输：**MCP底层采用JSON-RPC 2.0作为消息格式，所有请求和响应都以JSON结构封装【47:3†source】。传输层方面，MCP规范正式支持标准输入/输出（STDIO）和HTTP协议进行通信，并可选用Server-Sent Events（SSE）实现流式响应【47:3†source】。这意味着MCP既可在本地通过管道与模型交互（如在桌面应用中），也可通过网络HTTP请求与远程服务通信，具有部署灵活性。每个MCP消息都明确定义了方法名称和参数，其中**工具调用**相关的方法如前述的`tools/list`、`tools/call`等。此外，协议还定义了**通知**（如工具列表变更通知）和错误处理机制，确保客户端和服务器之间状态同步和异常传达。

**架构与组件：**MCP的实现由多个组件协同构成。Anthropic官方提供了详细的协议规范（schema定义）和多语言SDK，开发者可用这些SDK快速构建MCP客户端或服务器 。MCP服务器需要注册其提供的工具功能，每个工具以**唯一名称**标识，并携带输入/输出模式等元数据，这通常通过声明JSON Schema来描述参数结构【47:6†source】,【47:6†source】。开发者在服务器实现中，将工具名与实际执行函数或API调用关联，当服务器收到对应`tools/call`请求时触发执行。MCP客户端则嵌入在LLM应用中，负责维护与多个服务器的会话（如通过WebSocket长连接保持会话），管理工具列表缓存，以及根据模型请求发起调用并返回结果。值得一提的是，**能力协商**机制使客户端和服务器在连接时相互告知支持的功能集，例如服务器声明是否支持工具动态增减通知等【47:6†source】。这类似于LSP的能力交换，确保双方对协议扩展（如新增内容类型、取消操作等）达成一致。

**技术栈与生态：**由于MCP定位为开放标准，目前已经出现多种语言的参考实现和扩展工具。例如，Azure Functions推出了MCP扩展（预览版），允许开发者用云函数快速构建可扩展的远程MCP服务器，实现云端工具的快速发布,。又如，有社区项目提供了Spring框架的MCP支持，简化Java环境下MCP服务的开发。Anthropic官方开源了多个常用MCP服务器实现（连接Google Drive、Slack、GitHub等）的代码，开发者可以直接部署或参考 。这些都体现出MCP以开放开源的方式快速走向生态繁荣。

**安全与权限控制：**考虑到MCP能够让AI访问敏感数据并执行外部操作，协议从设计上非常强调用户**知情和授权**。Anthropic在规范中规定：用户必须对数据访问和工具调用给予明确同意，应用需要为用户提供透明的界面来审查和授权AI所要执行的操作【47:5†source】。例如，当MCP客户端准备让模型调用一个可能更改用户数据的工具时，应提示用户确认。Hosts一侧在暴露任何用户数据给服务器前都要征得许可，并对传输的数据施加必要的访问控制【47:5†source】。在认证方面，MCP支持标准的**OAuth2**等令牌机制对接第三方服务【47:1†source】（官方MCP Connector已提供`authorization_token`参数以集成OAuth流程【47:4†source】），避免每个工具各自为政的认证方案。协议还建议对工具描述等信息默认不信任（除非来自可信服务器），以防止恶意工具误导AI行为【47:6†source】。整体而言，MCP在灵活开放的同时，通过规范化的身份认证和用户审批机制来保障安全，并将具体安全实现留给各应用结合自身场景采取最佳实践【47:5†source】。

## 优势与应用案例
**MCP的主要技术优势**体现在以下几个方面：

- **开放标准，生态兼容：**MCP不依赖任何单一厂商平台，遵循通用协议格式，**模型无关、工具无关**。任何支持MCP的LLM客户端都能调用任意MCP服务，打破以往一个模型对应一套插件的**孤岛模式**【47:1†source】,【47:1†source】。这使跨模型、多工具的混合应用成为可能，避免厂商锁定。
- **持久会话与上下文：**MCP采用有状态连接，可在多轮对话中保留工具使用上下文，实现连续复杂操作。这种持续性避免了每次调用重新建立连接和重复上下文传递的开销，提高了执行效率和稳定性【47:1†source】。相较之下，传统函数调用每次独立，无法让AI“记住”先前步骤的结果，而MCP的持续上下文使**链式推理**和**工具组合**成为可能。
- **统一接口与丰富功能：**MCP提供一致的接口规范，让各种类型的外部能力——文件读取、数据库查询、代码执行等——都通过**工具（Tool）**这一抽象呈现给模型【47:6†source】。协议除了承载函数调用，还定义了**资源**共享和**模版提示**等机制，支持将外部数据直接作为上下文提供给模型，以及让服务器引导模型执行特定多步操作【47:5†source】。因此MCP比简单的函数调用框架功能更强大、用途更广。
- **数据格式规范与安全控制：**相比插件等先行方案，MCP严格规范了消息格式和交互流程，**定义清晰的JSON模式和传输协议以及统一的认证流程**,。这带来了可预期的稳定性和易调试性。并且MCP从协议层面注重权限与安全，要求“**人类始终在环**”——任何工具的使用都需获得用户许可，使AI操作保持在可控范围【47:6†source】。标准化的OAuth集成和访问控制降低了敏感数据泄露的风险。
- **性能和开发效率：**通过长连接与统一协议，MCP减少了多工具集成的通信开销。当AI在多个数据源之间频繁交互时，不需要反复进行不同API格式的解析和调用，降低了延迟和出错概率。此外，开发者只需针对MCP标准实现一次接口，即可供各种模型使用，大幅减少重复工作量 , 。尤其在复杂企业环境下，MCP统一了“接口语言”，让工具开发与模型应用解耦，维护成本显著降低。

得益于以上优势，MCP正被快速应用到众多场景中：

- **开源工具集成：**Anthropic已开放**预构建MCP连接器**库，实现对常见平台的支持，包括Google Drive文件库、Slack消息系统、GitHub代码仓库、PostgreSQL数据库、浏览器控制（Puppeteer）等, 。开发者可以直接部署这些MCP服务器，使LLM能够即时访问这些服务。例如，通过GitHub MCP服务器，模型可执行文件检索、Issue创建、代码搜索等操作【47:4†source】。这种开源工具生态显著降低了AI应用接入第三方系统的门槛。
- **企业内部助理：**在企业场景下，MCP被用于构建连接内部知识库和业务系统的AI助手。例如区块(Block)公司将MCP集成到内部工具，AI助手能够安全地提取公司文档、客户关系管理（CRM）数据等用于回答员工查询, 。MCP提供的安全机制确保这些敏感访问在授权范围内进行。据统计，引入MCP后企业在集成多个模型和数据源时的开发成本下降显著，同时信息检索效率提升，使AI真正融入日常业务流程。
- **开发者工具链：**MCP在软件开发领域展示出独特价值。多个现代IDE和编程平台（如**Zed编辑器、Replit在线开发环境、Codeium智能编码工具和Sourcegraph代码搜索**）正在将MCP纳入其平台, 。通过MCP，这些工具可让AI编程助手实时获取项目代码、依赖库及文档等上下文，从而提供更准确的代码补全、错误调试和文档生成等支持。在所谓“*vibe coding*”的工作流中，开发者与AI助手协同编程，MCP保证了AI始终掌握当前项目的最新状态【47:3†source】。例如，当开发者编辑代码时，AI可用MCP读取相关文件内容并进行分析，再给出改进建议，减少了来回切换和人工提示的开销。
- **自然语言数据查询：**借助MCP，一些应用将LLM与结构化数据源无缝衔接，实现“问答式”的数据查询。比如AI2SQL项目通过MCP服务器连接数据库，使模型能将用户的自然语言问题转换为SQL查询获取答案【47:3†source】。研究者也将MCP用于学术工具中，将文献管理器（如Zotero）的检索和标注功能暴露给AI，AI助手可以通过MCP跨论文库进行语义搜索、提取注释并汇总见解，辅助文献综述撰写【47:3†source】。这些案例表明，MCP拓展了LLM的能力边界，让模型成为各种领域数据的“查询接口”，极大地丰富了AI应用的实用性。

## 与其他协议的比较
随着AI工具连接方案的涌现，有必要从功能、性能、兼容性等方面将MCP与其他代表性机制进行对比：

1. **OpenAI函数调用接口：**OpenAI于2023年发布的函数调用机制使GPT模型能够按照预先注册的函数schema生成调用请求。其优点是实现简单，适合封闭环境快速验证，但局限明显：该接口**紧耦合OpenAI平台**及其专有API格式，每个会话只能使用预定义的一组函数，且调用结果需要开发者额外编写代码执行并返回【47:1†source】。更重要的是，它一次调用完成后不保留状态，无法自然地进行多步交互。相较而言，MCP作为**模型无关的持久通道**克服了这些限制：任何支持MCP的模型都能调用任意工具，并在会话中保持上下文状态，适合更复杂的跨步任务【47:1†source】。当然，在原型开发阶段，函数调用的简单直接仍具有优势，初始集成成本低；但从长期看，这种封闭实现存在生态锁定风险和扩展瓶颈【47:2†source】,【47:2†source】。

2. **ChatGPT插件机制：**ChatGPT插件是OpenAI为ChatGPT推出的扩展方式，每个插件通过OpenAPI规范定义一套HTTP接口供模型调用。插件的创新之处在于赋予模型调用任意Web服务的能力，但**缺点**也十分突出：每个插件都是**独立定制**的，需要单独的schema和认证方式，模型与插件之间没有共享标准【47:1†source】；插件调用通常是一次性的，请求-响应式交互，没有持续会话（模型无法记忆先前插件结果）；而且插件只能在特定平台（如ChatGPT网页）使用，**其他环境下不可通用**。MCP则在这些方面全面改进：作为开放标准，它对所有AI平台开放采纳，并支持长连接保持对话状态，还制定了统一的**OAuth2**认证流程，增强安全性和用户体验【47:1†source】。正如业内分析所言，ChatGPT插件就像一个**封闭工具箱**中的专用工具，而MCP更像一个**开放标准的工具套件**，普通开发者也能为AI添加工具，且不同AI系统都可共享这一工具生态【47:1†source】。

3. **Agent工具框架：**在高层协议出现前，不少开发者采用**Agent**框架（如LangChain等）来连接LLM与外部工具。该方法通常通过在模型对话中设定特殊格式的“行动指令”（例如输出“Action: 搜索引擎”），由代理框架解析后调用相应API。这类方案虽然实用，但本质上是基于模型输出文本的“Hack”，**缺乏标准**：不同工具的指令格式各异，也没有统一的工具描述和错误处理规范，集成往往针对特定模型定制【47:1†source】。相比之下，MCP提供了**单一标准接口**来呈现工具，所有工具都有一致的声明方式（名称、参数模式等），使模型与工具交互被纳入统一的协议层处理【47:1†source】。工具调用的逻辑被集中在MCP协议本身，而不需要散落在各种代理代码中，从而**提高了可靠性和可维护性**。Agent框架在遇到MCP后也可融合使用——Agent负责决策哪一步需要用何种工具，MCP负责具体执行和结果传输，二者结合能实现更强的自动化流水线。

4. **直接API集成：**最朴素的方式是由开发者解析模型输出或用户请求，手工决定调用哪个外部API并将结果插入模型回复。这种方式不依赖任何通用协议，但**开发和运维成本极高**：每增加一个新API接口，都需要编写定制代码处理输入输出；多个API混合调用时流程复杂且容易出错；缺乏标准的错误处理和认证机制，不同接口的安全策略各不相同【47:1†source】。这类“烟囱式”集成无法规模化。MCP对此的解决之道是**标准化与可重用**：任一服务只要实现了MCP服务器，任何MCP兼容的AI客户端都能与其交互，而无需针对每个模型重复开发【47:1†source】。统一的协议也提供了通用的错误返回格式和认证流程，使得错误处理和权限校验可以框架化地解决。相对于手工整合，每接入一个新工具，通过MCP所需的增量工作大幅减少，从而让AI系统的扩展具备线性甚至常数级的成本增长，而非指数级。

综合来看，MCP在功能丰富度、持续交互能力、跨平台兼容性上都显著优于以往方案，但其引入也带来了**实现复杂度**的增加。对小规模、简单场景而言，设置MCP服务器和客户端可能初期投入更大，不如直接函数调用那样即插即用。然而，随着应用需求增长和工具数量扩展，MCP的优势会逐渐凸显。未来MCP协议本身还有改进空间，例如进一步简化部署流程（目前Anthropic正开发云端MCP服务托管工具包）、优化对不同编程语言的支持，以及提供更完善的权限管理模板等。这些改进将在**通用标准**与**使用便利**之间取得平衡。

## 总结与展望
MCP作为一项新兴的开放协议，为AI模型接入外部世界架起了统一桥梁。它诞生的时间不长，却已经展现出成为**行业标准**的潜力。Anthropic开源MCP的举措得到了广泛响应，OpenAI、Google DeepMind等重量级AI平台相继宣布支持该协议，将其整合进自家产品（如ChatGPT的应用接口等）【47:7†source】。这种快速的采纳表明业界对标准化AI集成的价值形成共识：**通用协议带来的长远收益远超短期的定制方便**。通过MCP，AI助理可以在不同工具和数据集之间**保持上下文连续性**，打破过去各工具间割裂的使用体验，构建更持续智能的应用逻辑 。

当然，MCP要真正成熟为生态基础设施，还需在实践中不断完善。**安全性**就是一个需持续关注的方面：随着AI获取权限范围扩大，如何确保数据隐私、访问控制和滥用防范变得尤为重要 。目前MCP通过用户授权和标准认证做了初步规范，未来可能引入更细粒度的权限模型和审计机制，以满足企业和监管的严格要求。此外，一些替代或补充协议也在探索中，例如统一意图协议（Unified Intent Protocol, UIM）等，它们侧重AI与Web服务的标准交互，强调意图表示和安全执行。这表明AI工具标准化仍在演进，MCP需要与时俱进，不断融汇社区智慧。值得庆幸的是，Anthropic采取了开放共建的态度，MCP项目已聚集起开发者社区的贡献力量。越来越多的开源实现和第三方工具正在丰富MCP生态，这将反过来促进更多AI厂商和用户选择采用MCP。

展望未来，随着基础模型能力的提升和MCP协议本身的优化，AI调用外部工具的**准确率和复杂任务完成度有望大幅提高**。如果MCP在部署便捷性、多语言支持、工具管理等方面持续改进，并通过社区合作构建出高质量、丰富多样的工具库，那么MCP极有可能成为连接AI系统与现实世界的**核心通用接口**。正如有行业专家所指出的，MCP的出现标志着AI生态迈向“USB-C时刻”——就像统一的USB标准之于硬件设备那样，MCP为AI应用提供了一个即插即用的万能连接器,。总而言之，MCP以其开放性和规范性满足了AI时代对于工具调用协议的迫切需求，在标准化道路上迈出了关键一步。展望其未来发展，我们有理由保持**谨慎的乐观**：在不断的技术打磨和生态扩展下，MCP有望成为新一代智能应用的底层支柱，推动AI更深地融入各行各业，为人类创造更智能便捷的工作生活体验。

## References
- [Specification - Model Context Protocol](https://modelcontextprotocol.io/specification/2025-06-18)
- [大模型中的MCP协议和Function Call有何区别？深度解析技术本质与实战选择 | BetterYeah AI智能体](https://www.betteryeah.com/blog/mcp-protocol-vs-function-call-differences)
- [Anthropic introduces open source Model Context Protocol to boost AI ...](https://www.techmonitor.ai/digital-economy/ai-and-automation/anthropic-introduces-open-source-mcp-to-simplify-ai-system-integrations)
- [万字长文，带你读懂 Anthropic MCP协议_梦朝思夕的技术博客_51CTO博客](https://blog.51cto.com/qiangmzsx/13635845)
- [Tools - Model Context Protocol](https://modelcontextprotocol.io/docs/concepts/tools)
- [Introducing the Model Context Protocol \ Anthropic](https://www.1337sheets.com/p/diving-deep-into-anthropics-model-context-protocol-mcp-power-promise-and-emerging-risks)
- [Model Context Protocol (MCP) vs Function Calling: A Deep Dive into AI ...](https://aifuturefront.com/model-context-protocol-mcp-vs-function-calling-a-deep-dive-into-ai-integration-architectures/)
